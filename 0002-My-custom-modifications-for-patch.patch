diff --git a/examples/model_configs/vllm_model_config.yaml b/examples/model_configs/vllm_model_config.yaml
index 66714a29..54058d56 100644
--- a/examples/model_configs/vllm_model_config.yaml
+++ b/examples/model_configs/vllm_model_config.yaml
@@ -1,13 +1,15 @@
 model_parameters:
-  model_name: "HuggingFaceTB/SmolLM2-1.7B-Instruct"
-  revision: "57aa3c6599c53705406c648e7acca7e11dc45ea3"
+  model_name: "/llm/shawn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
+  revision: "main"
+  distributed_executor_backend: "ray"
+  load_in_low_bit: "fp8"
   dtype: "float16"
   tensor_parallel_size: 1
   data_parallel_size: 1
   pipeline_parallel_size: 1
-  gpu_memory_utilization: 0.6
-  max_model_length: null
-  swap_space: 4
+  gpu_memory_utilization: 0.8
+  max_model_length: 35587
+  swap_space: 16
   seed: 42
   trust_remote_code: False
   use_chat_template: True
@@ -16,7 +18,7 @@ model_parameters:
   pairwise_tokenization: False
   subfolder: null
   max_num_seqs: 1
-  max_num_batched_tokens: 8192
+  max_num_batched_tokens: 35587
   is_async: false
   generation_parameters:
     presence_penalty: 0.0
@@ -28,7 +30,7 @@ model_parameters:
     top_p: 0.9
     seed: 42
     stop_tokens: null
-    max_new_tokens: 2048
+    max_new_tokens: 32768
     min_new_tokens: 0
 metrics_options:
-  yo: null
+  yo: null
\ No newline at end of file
diff --git a/src/lighteval/models/vllm/vllm_model.py b/src/lighteval/models/vllm/vllm_model.py
index b1de9d0a..62932a39 100644
--- a/src/lighteval/models/vllm/vllm_model.py
+++ b/src/lighteval/models/vllm/vllm_model.py
@@ -46,7 +46,8 @@ logger = logging.getLogger(__name__)
 if is_vllm_available():
     import ray
     from more_itertools import distribute
-    from vllm import LLM, RequestOutput, SamplingParams
+    from ipex_llm.vllm.xpu.engine import IPEXLLMClass as LLM
+    from vllm import RequestOutput, SamplingParams
     from vllm.distributed.parallel_state import (
         destroy_distributed_environment,
         destroy_model_parallel,
@@ -167,7 +168,8 @@ class VLLMModelConfig(ModelConfig):
     subfolder: str | None = None
     use_chat_template: bool = False
     is_async: bool = False  # Whether to use the async version or sync version of the model
-
+    distributed_executor_backend: str = "ray"
+    load_in_low_bit: str = "asym_int4"
 
 class VLLMModel(LightevalModel):
     def __init__(
@@ -251,6 +253,10 @@ class VLLMModel(LightevalModel):
             "seed": int(config.seed),
             "max_num_seqs": int(config.max_num_seqs),
             "max_num_batched_tokens": int(config.max_num_batched_tokens),
+            "load_in_low_bit": config.load_in_low_bit,
+            "disable_async_output_proc": True,
+            "enforce_eager": True,
+            "block_size": 8,
         }
 
         if config.quantization is not None:
@@ -540,7 +546,10 @@ class AsyncVLLMModel(VLLMModel):
             "seed": int(config.seed),
             "max_num_seqs": int(config.max_num_seqs),
             "max_num_batched_tokens": int(config.max_num_batched_tokens),
+            "load_in_low_bit": config.load_in_low_bit,
+            "disable_async_output_proc": True,
             "enforce_eager": True,
+            "block_size": 8,
         }
 
         if config.data_parallel_size > 1:
