From dcdc81c5792ce5f73ff9481b7d719409a2152480 Mon Sep 17 00:00:00 2001
From: Shawn Zhao <shawn.zhao@intel.com>
Date: Wed, 23 Jul 2025 11:58:54 +0800
Subject: [PATCH] My custom modifications for patch

---
 lm_eval/models/vllm_causallms.py | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

diff --git a/lm_eval/models/vllm_causallms.py b/lm_eval/models/vllm_causallms.py
index a9ce3a93..efabd82b 100644
--- a/lm_eval/models/vllm_causallms.py
+++ b/lm_eval/models/vllm_causallms.py
@@ -33,7 +33,8 @@ from lm_eval.utils import (
 
 try:
     import ray
-    from vllm import LLM, SamplingParams
+    from ipex_llm.vllm.xpu.engine import IPEXLLMClass as LLM
+    from vllm import SamplingParams
     from vllm.lora.request import LoRARequest
     from vllm.transformers_utils.tokenizer import get_tokenizer
     from vllm.utils import get_open_port
@@ -131,7 +132,7 @@ class VLLM(TemplateLM):
         max_model_len: int = None,
         seed: int = 1234,
         gpu_memory_utilization: float = 0.9,
-        device: str = "cuda",
+        device: str = "xpu",
         data_parallel_size: int = 1,
         lora_local_path: str = None,
         # VLLM: enable thinking tags in the prompt.
-- 
2.34.1

